"""
{{.project_name}} - Main Task
Generated by Databricks Asset Bundle Template
{{- if eq .enable_unity_catalog "yes"}}
Unity Catalog: {{.catalog_name}}.{{.schema_name}}
{{- end}}
"""

import sys
import argparse
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='{{.project_name}} main task')
    {{- if eq .enable_unity_catalog "yes"}}
    parser.add_argument('--catalog', default='{{.catalog_name}}', help='Unity Catalog catalog name')
    parser.add_argument('--schema', default='{{.schema_name}}', help='Unity Catalog schema name')
    {{- end}}
    parser.add_argument('--env', default='dev', help='Environment (dev/staging/prod)')
    return parser.parse_args()

def main():
    """Main execution function"""
    args = parse_args()

    # Create Spark session
    spark = SparkSession.builder \
        .appName('{{.project_name}}') \
        .getOrCreate()

    print(f'Starting {{.project_name}} job')
    print(f'Spark version: {spark.version}')
    print(f'Environment: {args.env}')

    {{- if eq .enable_unity_catalog "yes"}}
    # Unity Catalog configuration
    catalog_path = f"{args.catalog}.{args.schema}"
    print(f'Using Unity Catalog: {catalog_path}')

    # Set current catalog and schema
    spark.sql(f"USE CATALOG {args.catalog}")
    spark.sql(f"USE SCHEMA {args.schema}")

    # Example: Create a sample table
    sample_data = spark.range(0, 100).select(
        F.col("id"),
        F.rand().alias("random_value"),
        F.current_timestamp().alias("created_at")
    )

    # Write to Unity Catalog table
    table_name = f"{catalog_path}.sample_data_{args.env}"
    sample_data.write \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(table_name)

    print(f'Successfully wrote data to {table_name}')

    # Read and display sample
    df = spark.table(table_name)
    print(f'Table row count: {df.count()}')
    df.show(10)
    {{- else}}
    # Using Hive metastore
    print('Using Hive metastore')

    # Example: Create a sample DataFrame
    sample_data = spark.range(0, 100).select(
        F.col("id"),
        F.rand().alias("random_value"),
        F.current_timestamp().alias("created_at")
    )

    # Write to table
    table_name = f"{{.project_name}}_sample_data_{args.env}"
    sample_data.write \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(table_name)

    print(f'Successfully wrote data to {table_name}')

    # Read and display sample
    df = spark.table(table_name)
    print(f'Table row count: {df.count()}')
    df.show(10)
    {{- end}}

    print('{{.project_name}} job completed successfully')
    spark.stop()

if __name__ == '__main__':
    main()
