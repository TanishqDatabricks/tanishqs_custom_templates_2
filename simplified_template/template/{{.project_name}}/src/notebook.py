# Databricks notebook source
# MAGIC %md
# MAGIC # {{.project_name}} - Notebook Task
# MAGIC
# MAGIC Generated by Databricks Asset Bundle Template
# MAGIC {{- if eq .enable_unity_catalog "yes"}}
# MAGIC
# MAGIC **Unity Catalog:** `{{.catalog_name}}.{{.schema_name}}`
# MAGIC {{- end}}
# MAGIC
# MAGIC **Cloud Provider:** {{.cloud_provider}}
# MAGIC
# MAGIC **Compute Type:** {{.compute_type}}

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup

# COMMAND ----------

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Get widget values (parameters)
{{- if eq .enable_unity_catalog "yes"}}
dbutils.widgets.text("catalog", "{{.catalog_name}}", "Catalog")
dbutils.widgets.text("schema", "{{.schema_name}}", "Schema")
{{- end}}
dbutils.widgets.text("env", "dev", "Environment")

{{- if eq .enable_unity_catalog "yes"}}
catalog = dbutils.widgets.get("catalog")
schema = dbutils.widgets.get("schema")
{{- end}}
env = dbutils.widgets.get("env")

print(f"Environment: {env}")
{{- if eq .enable_unity_catalog "yes"}}
print(f"Catalog: {catalog}")
print(f"Schema: {schema}")
{{- end}}

# COMMAND ----------

# MAGIC %md
# MAGIC ## Data Processing

# COMMAND ----------

{{- if eq .enable_unity_catalog "yes"}}
# Set Unity Catalog context
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE SCHEMA {schema}")

# Create sample data
df = spark.range(0, 1000).select(
    F.col("id"),
    F.rand().alias("value"),
    (F.col("id") % 10).alias("category"),
    F.current_timestamp().alias("timestamp")
)

# Write to Unity Catalog table
table_name = f"{catalog}.{schema}.notebook_data_{env}"
{{- else}}
# Create sample data
df = spark.range(0, 1000).select(
    F.col("id"),
    F.rand().alias("value"),
    (F.col("id") % 10).alias("category"),
    F.current_timestamp().alias("timestamp")
)

# Write to Hive metastore table
table_name = f"{{.project_name}}_notebook_data_{env}"
{{- end}}

df.write \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable(table_name)

print(f"Wrote {df.count()} rows to {table_name}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Analytics

# COMMAND ----------

# Read the data back
result_df = spark.table(table_name)

# Perform aggregations
summary = result_df.groupBy("category").agg(
    F.count("*").alias("count"),
    F.avg("value").alias("avg_value"),
    F.min("value").alias("min_value"),
    F.max("value").alias("max_value")
).orderBy("category")

display(summary)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Results Summary

# COMMAND ----------

print(f"âœ… Successfully processed data in {table_name}")
print(f"ðŸ“Š Total rows: {result_df.count()}")
print(f"ðŸ“ˆ Categories: {result_df.select('category').distinct().count()}")
